import math

def learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str = 'PlayTennis') -> dict:
    # Base cases
    # If all examples have the same target value, return that value
    target_values = [example[target_attr] for example in examples]
    if len(set(target_values)) == 1:
        return target_values[0]
    
    # If no attributes left, return the majority target value
    if not attributes:
        majority_value = max(set(target_values), key=target_values.count)
        return majority_value
    
    # Select the best attribute to split on
    best_attribute = select_best_attribute(examples, attributes, target_attr)
    tree = {best_attribute: {}}
    
    # Get all unique values of the best attribute in the examples
    values = set(example[best_attribute] for example in examples)
    
    # Remove the best attribute from the list of attributes
    remaining_attributes = [attr for attr in attributes if attr != best_attribute]
    
    # Recursively build the tree for each value of the best attribute
    for value in values:
        subset = [example for example in examples if example[best_attribute] == value]
        if not subset:
            # If no examples for this value, return the majority target value
            majority_value = max(set(target_values), key=target_values.count)
            tree[best_attribute][value] = majority_value
        else:
            subtree = learn_decision_tree(subset, remaining_attributes, target_attr)
            tree[best_attribute][value] = subtree
    
    return tree

def select_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:
    # Calculate the entropy of the target attribute
    entropy_before = calculate_entropy(examples, target_attr)
    max_info_gain = -1
    best_attribute = None
    
    for attribute in attributes:
        # Calculate information gain for this attribute
        info_gain = calculate_information_gain(examples, attribute, target_attr, entropy_before)
        if info_gain > max_info_gain:
            max_info_gain = info_gain
            best_attribute = attribute
    
    return best_attribute

def calculate_entropy(examples: list[dict], target_attr: str) -> float:
    target_values = [example[target_attr] for example in examples]
    value_counts = {}
    for value in target_values:
        value_counts[value] = value_counts.get(value, 0) + 1
    entropy = 0.0
    total = len(examples)
    for count in value_counts.values():
        probability = count / total
        entropy -= probability * math.log2(probability)
    return entropy

def calculate_information_gain(examples: list[dict], attribute: str, target_attr: str, entropy_before: float) -> float:
    # Get all unique values of the attribute
    values = set(example[attribute] for example in examples)
    weighted_entropy = 0.0
    total = len(examples)
    
    for value in values:
        subset = [example for example in examples if example[attribute] == value]
        subset_entropy = calculate_entropy(subset, target_attr)
        weighted_entropy += (len(subset) / total) * subset_entropy
    
    information_gain = entropy_before - weighted_entropy
    return information_gain
